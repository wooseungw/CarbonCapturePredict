{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 트랜스포머 모델의 개괄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 트랜스포머란?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽이 디코더(Decoder) 레이어, 오른쪽이 인코더(Encoder) 레이어\n",
    "\n",
    "셀프 어텐션과 멀티 헤드 어텐션을 수행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 어텐션 메커니즘(Attention Mechanism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 트랜스포머 구조를 이해하기 위해 **어텐션 메커니즘**을 이해할 필요가 있음\n",
    "\n",
    "CNN을 알기위해 Convolution을 알아야 하는것 처럼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. 기본 아이디어\n",
    "\n",
    "디코더에서 출력 예측하는 시점마다 인코더의 전체 입력을 다시 참고함\n",
    "\n",
    "여기서 전체 입력 중 예측하는 부분과 연관된 부분에 더 집중(Attenrion)해서 보기 때문에 \n",
    "\n",
    "    어텐션 메커니즘(Attention Mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 구성요소\n",
    "\n",
    "\n",
    "    쿼리(Query), 키(Key), 벨류(Value)\n",
    "\n",
    "위의 3가지 구성요소는 입력데이터($x$)에 대한 \"선형변환\" **(벡터와 행렬의 곱) $ = [vector][matrix]$** 으로 생성됨\n",
    "$$Q(Query) = xW_Q$$\n",
    "$$K(Key) = xW_K$$\n",
    "$$V(Value) = xW_V$$\n",
    "\n",
    "$W_Q,W_K,W_V$는 가중치 행렬이며 모델이 학습하며 업데이트하는 값\n",
    "\n",
    "$W_Q,W_K,W_V$는 임베딩 차원($d_{model}$)과 헤드의 개수($num\\;head$)에 기반해 생성\n",
    "\n",
    "임베딩 차원과 헤드가 뭔지는 트랜스포머 모델을 설명할 때\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 임베딩\n",
    "입력 데이터에 대한 고차원의 연속적인 특징 공간\n",
    "\"The cat sat on the mat\"를 입력 데이터라 할 때 $d_{model} = 512$ 라고 하자\n",
    "\n",
    "입력데이터에 대한 임베딩 결과는 $x.shape = [6,512] = [입력데이터의 길이,d_{model}]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 멀티-헤드\n",
    "\n",
    "트렌스포머 모델은 입력 데이터를 여러 헤드에서 동시에 처리함\n",
    "각 헤드마다 다른 관점에서 분석\n",
    "\n",
    "입력 데이터에 임베딩에는 사용되지 않음\n",
    "\n",
    "쿼리,키,벨류의 가중치 행렬 생성시 $shape = [d_{model},{d_{model} \\over num\\;head}]$으로 생성함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 쿼리(Query)\n",
    "\n",
    "현재 단어를 나타내는 벡터를 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 행렬 $W_Q = $\n",
    "\n",
    "$$W_Q = \\begin{bmatrix}\n",
    "q_{11} & \\cdots & q_{1(num\\;head)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_{(d_{model})1} & \\cdots & q_{(d_{model})(num\\;head)}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "가중치 행렬의 크기\n",
    "$$W_Q.shape = [d_{model},{d_{model} \\over num\\;head}]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예시\n",
    "\"The cat sat on the mat\"에서 \"cat\"이라는 단어를 분석한다고 가정할 때,$d_{model} = 512,num\\;head=8 $ 이라고 하자\n",
    "\n",
    " \"cat\"에 대응되는 **쿼리 벡터**를 찾고싶다.\n",
    "\n",
    "\n",
    "\n",
    "입력 데이터의 임베딩($x$)를 $x = \\begin{bmatrix}x_{1,1} & \\cdots & x_{1,512}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{6,1} & \\cdots & x_{6,512}\\end{bmatrix}$라 하자,\n",
    "\n",
    " 임의의 $W_Q = \\begin{bmatrix}\n",
    "q_{1,1} & \\cdots & q_{1,64}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_{512,1} & \\cdots & q_{512,64}\\\\\n",
    "\\end{bmatrix}$가 있을 때,\n",
    " \n",
    "  선형변환($=xW_Q$) 된 결과\n",
    "  \n",
    "$$Q =\n",
    "\\begin{bmatrix}x_{1,1} & \\cdots & x_{1,512}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{6,1} & \\cdots & x_{6,512}\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "q_{1,1} & \\cdots & q_{1,64}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_{512,1} & \\cdots & q_{512,64}\\\\\n",
    "\\end{bmatrix} \n",
    "=\\begin{bmatrix}\n",
    "(x_{1,1})(q_{1,1}) +\\cdots + (x_{1,512})(q_{512,1})& \\cdots & (x_{1,1})(q_{1,64}) +\\cdots +(x_{1,512}q_{512,64}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "(x_{6,1})(q_{1,1}) +\\cdots + (x_{1,512})(q_{512,1})& \\cdots & (x_{6,1})(q_{1,64}) +\\cdots + (x_{6,512})(q_{512,64})\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$Q.shape = [6,64] = [입력데이터길이,{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "$$\\begin{bmatrix} (x_{2,1})(q_{1,2}) +\\cdots + (x_{2,512})(q_{512,2}) \\\\ \\vdots \\\\ (x_{2,1})(q_{1,1}) +\\cdots + (x_{2,512})(q_{512,1})\\end{bmatrix}\\;이\\;\"cat\"에\\;대한\\;쿼리 벡터\\; 이다.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 키(Key) - 벨류(Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본개념: 키(Key) - 벨류(Value) 구조는 흔히 아래와 같은 딕셔너리 형에서 사용됨\n",
    "\n",
    "    키값으로 벨류를 가져올 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n"
     ]
    }
   ],
   "source": [
    "dict = {\"2017\" : \"Transformer\", \"2018\" : \"BERT\"}\n",
    "print(dict[\"2018\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "키(Key)가중치 행렬 \n",
    "\n",
    "$$W_K = \\begin{bmatrix}\n",
    "k_{11} & \\cdots & k_{1(num\\;head)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k_{(d_{model})1} & \\cdots & k_{(d_{model})(num\\;head)}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "가중치 행렬의 크기\n",
    "$$W_K.shape = [d_{model},{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "벨류(Value)가중치 행렬 \n",
    "\n",
    "$$W_V = \\begin{bmatrix}\n",
    "v_{11} & \\cdots & v_{1(num\\;head)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "v_{(d_{model})1} & \\cdots & v_{(d_{model})(num\\;head)}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "가중치 행렬의 크기\n",
    "$$W_K.shape = [d_{model},{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "#### 쿼리,키,벨류의 가중치 행렬은 모두 같은 크기를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Attention Score 계산\n",
    "\n",
    "입력 시퀀스(=현재 쿼리)의 대한 전체요소들과의 가중치를 계산하는 것이다.\n",
    "\n",
    "이 계산 방법에 따라 다양한 방법으로 구현 될 수 있다. 그 중 내적(Dot-Product)을 이용한 계산을 알아보자면\n",
    "$$ Attention(Q,K,V) = softmax({QK^T \\over \\sqrt{d_k}}) $$\n",
    "로 정의되며 싱글 헤드 어텐션과 멀티 헤드 어텐션일때 계산이 달라진다.\n",
    "\n",
    " - **멀티-헤드 어텐션**\n",
    "$$ Attention(Q,K,V) = softmax({QK^T \\over \\sqrt{d_k}}) = softmax({QK^T \\over \\sqrt{{d_{model} \\over num\\;head}}})$$\n",
    "\n",
    " - 싱글-헤드셀프 어텐션(별로 중요하지 않아보인다. 이때는 위의 가중치 행렬의 모양또한 바뀔것이다.) \n",
    " $$ = softmax({QK^T \\over \\sqrt{d_{model}}}) = softmax({QK^T \\over \\sqrt{{d_{model} \\over 1}}})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 트랜스포머 모델\n",
    "\n",
    "앞서 설명한 어텐션 개념만으로 작동함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 임베딩 차원과 멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q: torch.Size([32, 10, 64])\n",
      "Shape of K: torch.Size([32, 100, 64])\n",
      "Shape of V: torch.Size([32, 100, 32])\n",
      "torch.Size([32, 10, 32])\n",
      "<built-in method size of Tensor object at 0x000001C2E068C640>\n",
      "tensor([[0.0023, 0.0037, 0.0115, 0.0196, 0.0101, 0.0025, 0.0063, 0.0114, 0.0012,\n",
      "         0.0053, 0.0053, 0.0054, 0.0004, 0.0393, 0.0050, 0.0042, 0.0126, 0.0063,\n",
      "         0.0117, 0.0041, 0.0088, 0.0091, 0.0198, 0.0061, 0.0021, 0.0101, 0.0032,\n",
      "         0.0005, 0.0024, 0.0013, 0.0033, 0.0163, 0.0086, 0.0063, 0.0048, 0.0636,\n",
      "         0.0076, 0.0051, 0.0042, 0.0061, 0.0032, 0.0054, 0.0063, 0.0010, 0.0369,\n",
      "         0.0032, 0.0040, 0.0061, 0.0149, 0.0032, 0.0052, 0.0087, 0.0046, 0.0399,\n",
      "         0.0031, 0.0144, 0.0013, 0.0353, 0.0086, 0.0003, 0.0014, 0.0055, 0.0063,\n",
      "         0.0044, 0.0023, 0.0091, 0.0021, 0.0012, 0.0039, 0.0006, 0.0090, 0.0156,\n",
      "         0.0060, 0.0131, 0.0112, 0.0231, 0.0057, 0.0048, 0.0038, 0.0030, 0.0017,\n",
      "         0.0873, 0.0009, 0.0035, 0.0157, 0.0085, 0.0055, 0.0025, 0.0069, 0.0010,\n",
      "         0.0204, 0.0100, 0.0532, 0.0010, 0.0201, 0.0015, 0.0173, 0.0336, 0.0138,\n",
      "         0.0005],\n",
      "        [0.0058, 0.0074, 0.0103, 0.0006, 0.0115, 0.0135, 0.0039, 0.0034, 0.0067,\n",
      "         0.0075, 0.0044, 0.0190, 0.0024, 0.0012, 0.0102, 0.0063, 0.0102, 0.0045,\n",
      "         0.0038, 0.0048, 0.0082, 0.0093, 0.0179, 0.0058, 0.0045, 0.0204, 0.0297,\n",
      "         0.0076, 0.0069, 0.0045, 0.0061, 0.0250, 0.0129, 0.0416, 0.0105, 0.0036,\n",
      "         0.0230, 0.0018, 0.0046, 0.0051, 0.0243, 0.0031, 0.0059, 0.0109, 0.0133,\n",
      "         0.0089, 0.0128, 0.0138, 0.0169, 0.0074, 0.0160, 0.0040, 0.0367, 0.0070,\n",
      "         0.0030, 0.0066, 0.0053, 0.0125, 0.0047, 0.0302, 0.0022, 0.0110, 0.0113,\n",
      "         0.0026, 0.0054, 0.0009, 0.0099, 0.0098, 0.0145, 0.0054, 0.0045, 0.0024,\n",
      "         0.0015, 0.0036, 0.0047, 0.0056, 0.0129, 0.0191, 0.0060, 0.0056, 0.0053,\n",
      "         0.0112, 0.0115, 0.0132, 0.0081, 0.0006, 0.0297, 0.0197, 0.0071, 0.0079,\n",
      "         0.0261, 0.0060, 0.0013, 0.0100, 0.0051, 0.0073, 0.0184, 0.0108, 0.0150,\n",
      "         0.0174],\n",
      "        [0.0024, 0.0020, 0.0042, 0.0079, 0.0065, 0.0127, 0.0141, 0.0021, 0.0014,\n",
      "         0.0065, 0.0040, 0.0032, 0.0069, 0.0092, 0.0004, 0.0239, 0.0020, 0.0323,\n",
      "         0.0057, 0.0179, 0.0045, 0.0171, 0.0064, 0.0038, 0.0061, 0.0006, 0.0047,\n",
      "         0.0018, 0.0157, 0.0035, 0.0036, 0.0008, 0.0474, 0.0026, 0.0128, 0.0033,\n",
      "         0.0045, 0.0082, 0.0141, 0.0016, 0.0285, 0.0063, 0.0118, 0.0128, 0.0018,\n",
      "         0.0065, 0.0059, 0.0126, 0.0295, 0.0126, 0.0323, 0.0061, 0.0054, 0.0062,\n",
      "         0.0051, 0.0018, 0.0133, 0.0122, 0.0436, 0.0078, 0.0441, 0.0458, 0.0020,\n",
      "         0.0116, 0.0340, 0.0041, 0.0110, 0.0180, 0.0059, 0.0009, 0.0015, 0.0124,\n",
      "         0.0006, 0.0015, 0.0080, 0.0037, 0.0019, 0.0021, 0.0124, 0.0054, 0.0063,\n",
      "         0.0057, 0.0164, 0.0098, 0.0009, 0.0068, 0.0022, 0.0094, 0.0255, 0.0313,\n",
      "         0.0031, 0.0317, 0.0105, 0.0016, 0.0024, 0.0033, 0.0046, 0.0042, 0.0023,\n",
      "         0.0048],\n",
      "        [0.0049, 0.0029, 0.0030, 0.0066, 0.0133, 0.0036, 0.0085, 0.0050, 0.0178,\n",
      "         0.0029, 0.0088, 0.0065, 0.0085, 0.0163, 0.0064, 0.0166, 0.0301, 0.0092,\n",
      "         0.0058, 0.0119, 0.0007, 0.0179, 0.0070, 0.0013, 0.0054, 0.0025, 0.0162,\n",
      "         0.0085, 0.0012, 0.0031, 0.0209, 0.0413, 0.0066, 0.0022, 0.0178, 0.0012,\n",
      "         0.0200, 0.0148, 0.0132, 0.0048, 0.0017, 0.0059, 0.0056, 0.0077, 0.0223,\n",
      "         0.0292, 0.0211, 0.0022, 0.0037, 0.0033, 0.0042, 0.0111, 0.0005, 0.0090,\n",
      "         0.0017, 0.0078, 0.0022, 0.0180, 0.0035, 0.0047, 0.0027, 0.0189, 0.0013,\n",
      "         0.0102, 0.0123, 0.0046, 0.0184, 0.0120, 0.0029, 0.0022, 0.0086, 0.0172,\n",
      "         0.0132, 0.0014, 0.0019, 0.0063, 0.0041, 0.0012, 0.0467, 0.0170, 0.0053,\n",
      "         0.0670, 0.0122, 0.0120, 0.0288, 0.0051, 0.0039, 0.0037, 0.0144, 0.0129,\n",
      "         0.0040, 0.0058, 0.0077, 0.0196, 0.0031, 0.0008, 0.0025, 0.0099, 0.0080,\n",
      "         0.0096],\n",
      "        [0.0061, 0.0096, 0.0024, 0.0116, 0.0094, 0.0215, 0.0083, 0.0044, 0.0047,\n",
      "         0.0131, 0.0064, 0.0430, 0.0007, 0.0324, 0.0039, 0.0086, 0.0255, 0.0101,\n",
      "         0.0052, 0.0110, 0.0060, 0.0144, 0.0039, 0.0039, 0.0019, 0.0163, 0.0044,\n",
      "         0.0024, 0.0090, 0.0059, 0.0100, 0.0142, 0.0035, 0.0068, 0.0185, 0.0018,\n",
      "         0.0136, 0.0106, 0.0172, 0.0146, 0.0026, 0.0589, 0.0123, 0.0128, 0.0056,\n",
      "         0.0046, 0.0069, 0.0084, 0.0210, 0.0065, 0.0049, 0.0077, 0.0041, 0.0097,\n",
      "         0.0113, 0.0100, 0.0046, 0.0130, 0.0181, 0.0076, 0.0026, 0.0066, 0.0051,\n",
      "         0.0023, 0.0018, 0.0044, 0.0032, 0.0006, 0.0057, 0.0007, 0.0064, 0.0187,\n",
      "         0.0135, 0.0069, 0.0084, 0.0134, 0.0020, 0.0057, 0.0166, 0.0031, 0.0087,\n",
      "         0.0164, 0.0510, 0.0015, 0.0050, 0.0129, 0.0032, 0.0022, 0.0122, 0.0022,\n",
      "         0.0053, 0.0208, 0.0064, 0.0378, 0.0075, 0.0020, 0.0116, 0.0154, 0.0018,\n",
      "         0.0010],\n",
      "        [0.0031, 0.0189, 0.0018, 0.0023, 0.0013, 0.0015, 0.0020, 0.0006, 0.0020,\n",
      "         0.0081, 0.0059, 0.0016, 0.0039, 0.0002, 0.0012, 0.0048, 0.0193, 0.0043,\n",
      "         0.0017, 0.0009, 0.0097, 0.0024, 0.0179, 0.0103, 0.0019, 0.0205, 0.0036,\n",
      "         0.0065, 0.0052, 0.0047, 0.0060, 0.0012, 0.0074, 0.0107, 0.0228, 0.0125,\n",
      "         0.0315, 0.0565, 0.0086, 0.0397, 0.0014, 0.0023, 0.0096, 0.0042, 0.0002,\n",
      "         0.0084, 0.0054, 0.0032, 0.0034, 0.0520, 0.0081, 0.0005, 0.0021, 0.0013,\n",
      "         0.0051, 0.0124, 0.0010, 0.0065, 0.0054, 0.0016, 0.0029, 0.0185, 0.0274,\n",
      "         0.0004, 0.0014, 0.0012, 0.0052, 0.0157, 0.0052, 0.0083, 0.0045, 0.0078,\n",
      "         0.0014, 0.0255, 0.0038, 0.0088, 0.0007, 0.0036, 0.0100, 0.0388, 0.0022,\n",
      "         0.0093, 0.0009, 0.0039, 0.0091, 0.0306, 0.0101, 0.0010, 0.0288, 0.0026,\n",
      "         0.0067, 0.0198, 0.0039, 0.0043, 0.0078, 0.0099, 0.1351, 0.0142, 0.0036,\n",
      "         0.0060],\n",
      "        [0.0107, 0.0107, 0.0329, 0.0075, 0.0055, 0.0074, 0.0114, 0.0040, 0.0080,\n",
      "         0.0097, 0.0295, 0.0209, 0.0103, 0.0027, 0.0027, 0.0294, 0.0188, 0.0064,\n",
      "         0.0042, 0.0082, 0.0034, 0.0160, 0.0026, 0.0023, 0.0006, 0.0113, 0.0037,\n",
      "         0.0211, 0.0015, 0.0045, 0.0053, 0.0068, 0.0046, 0.0358, 0.0164, 0.0130,\n",
      "         0.0046, 0.0036, 0.0030, 0.0082, 0.0032, 0.0058, 0.0041, 0.0066, 0.0172,\n",
      "         0.0178, 0.0081, 0.0061, 0.0016, 0.0608, 0.0229, 0.0046, 0.0015, 0.0054,\n",
      "         0.0163, 0.0349, 0.0040, 0.0029, 0.0070, 0.0017, 0.0012, 0.0075, 0.0122,\n",
      "         0.0016, 0.0094, 0.0036, 0.0057, 0.0038, 0.0097, 0.0162, 0.0321, 0.0061,\n",
      "         0.0099, 0.0078, 0.0018, 0.0036, 0.0019, 0.0091, 0.0176, 0.0017, 0.0100,\n",
      "         0.0240, 0.0075, 0.0108, 0.0076, 0.0071, 0.0056, 0.0050, 0.0109, 0.0029,\n",
      "         0.0033, 0.0079, 0.0016, 0.0325, 0.0025, 0.0160, 0.0130, 0.0239, 0.0015,\n",
      "         0.0019],\n",
      "        [0.0147, 0.0022, 0.0171, 0.0309, 0.0287, 0.0025, 0.0079, 0.0084, 0.0049,\n",
      "         0.0152, 0.0024, 0.0057, 0.0080, 0.0012, 0.0156, 0.0077, 0.0358, 0.0083,\n",
      "         0.0018, 0.0291, 0.0343, 0.0066, 0.0107, 0.0031, 0.0041, 0.0135, 0.0060,\n",
      "         0.0062, 0.0196, 0.0433, 0.0025, 0.0057, 0.0280, 0.0218, 0.0152, 0.0080,\n",
      "         0.0125, 0.0033, 0.0126, 0.0059, 0.0059, 0.0043, 0.0027, 0.0133, 0.0041,\n",
      "         0.0048, 0.0117, 0.0110, 0.0172, 0.0063, 0.0060, 0.0046, 0.0052, 0.0091,\n",
      "         0.0036, 0.0133, 0.0023, 0.0110, 0.0015, 0.0028, 0.0013, 0.0035, 0.0269,\n",
      "         0.0043, 0.0125, 0.0011, 0.0294, 0.0074, 0.0047, 0.0123, 0.0120, 0.0078,\n",
      "         0.0023, 0.0065, 0.0148, 0.0041, 0.0079, 0.0021, 0.0028, 0.0273, 0.0016,\n",
      "         0.0014, 0.0022, 0.0046, 0.0044, 0.0072, 0.0077, 0.0117, 0.0075, 0.0116,\n",
      "         0.0147, 0.0249, 0.0031, 0.0083, 0.0050, 0.0186, 0.0044, 0.0032, 0.0067,\n",
      "         0.0084],\n",
      "        [0.0007, 0.0061, 0.0083, 0.0034, 0.0115, 0.0048, 0.0139, 0.0058, 0.0113,\n",
      "         0.0007, 0.0075, 0.0095, 0.0008, 0.0007, 0.0058, 0.0027, 0.0065, 0.0026,\n",
      "         0.0144, 0.0041, 0.0014, 0.0144, 0.0014, 0.0038, 0.0056, 0.0067, 0.0078,\n",
      "         0.0114, 0.0023, 0.0299, 0.0053, 0.0173, 0.0079, 0.0493, 0.0950, 0.0033,\n",
      "         0.0360, 0.0376, 0.0089, 0.0095, 0.0057, 0.0184, 0.0029, 0.0104, 0.0023,\n",
      "         0.0373, 0.0069, 0.0067, 0.0023, 0.0043, 0.0168, 0.0018, 0.0007, 0.0035,\n",
      "         0.0104, 0.0123, 0.0049, 0.0030, 0.0024, 0.0070, 0.0068, 0.0053, 0.0102,\n",
      "         0.0014, 0.0042, 0.0015, 0.0104, 0.0018, 0.0005, 0.0066, 0.0114, 0.0023,\n",
      "         0.0156, 0.0126, 0.0398, 0.0197, 0.0032, 0.0019, 0.0048, 0.0068, 0.0020,\n",
      "         0.0005, 0.0024, 0.0033, 0.0021, 0.0124, 0.0184, 0.0029, 0.0105, 0.0015,\n",
      "         0.0010, 0.0430, 0.0043, 0.0057, 0.0016, 0.0600, 0.0112, 0.0115, 0.0018,\n",
      "         0.0015],\n",
      "        [0.0047, 0.0296, 0.0135, 0.0072, 0.0023, 0.0296, 0.0322, 0.0357, 0.0046,\n",
      "         0.0064, 0.0122, 0.0016, 0.0144, 0.0041, 0.0136, 0.0040, 0.0021, 0.0062,\n",
      "         0.0022, 0.0034, 0.0105, 0.0101, 0.0074, 0.0199, 0.0076, 0.0011, 0.0044,\n",
      "         0.0170, 0.0153, 0.0097, 0.0032, 0.0131, 0.0036, 0.0018, 0.0021, 0.0102,\n",
      "         0.0014, 0.0020, 0.0098, 0.0009, 0.0099, 0.0017, 0.0036, 0.0073, 0.0109,\n",
      "         0.0085, 0.0054, 0.0276, 0.0171, 0.0273, 0.0036, 0.0031, 0.0161, 0.0026,\n",
      "         0.0034, 0.0041, 0.0055, 0.0036, 0.0226, 0.0143, 0.0176, 0.0065, 0.0042,\n",
      "         0.0435, 0.0109, 0.0125, 0.0041, 0.0171, 0.0015, 0.0025, 0.0049, 0.0131,\n",
      "         0.0042, 0.0076, 0.0031, 0.0035, 0.0124, 0.0167, 0.0065, 0.0107, 0.0306,\n",
      "         0.0023, 0.0109, 0.0171, 0.0012, 0.0030, 0.0048, 0.0114, 0.0092, 0.0074,\n",
      "         0.0635, 0.0015, 0.0069, 0.0195, 0.0033, 0.0056, 0.0026, 0.0012, 0.0133,\n",
      "         0.0026]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    어텐션 점수, 가중치, 그리고 출력을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "    - query (torch.Tensor): 쿼리 텐서, 크기는 (batch_size, num_queries, d_k).\n",
    "    - key (torch.Tensor): 키 텐서, 크기는 (batch_size, num_keys, d_k).\n",
    "    - value (torch.Tensor): 값 텐서, 크기는 (batch_size, num_keys, d_v).\n",
    "    - mask (torch.Tensor, optional): 마스크 텐서, 크기는 (batch_size, num_queries, num_keys).\n",
    "    \n",
    "    Returns:\n",
    "    - output (torch.Tensor): 출력 텐서, 크기는 (batch_size, num_queries, d_v).\n",
    "    - attention_weights (torch.Tensor): 어텐션 가중치, 크기는 (batch_size, num_queries, num_keys).\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 어텐션 점수 계산\n",
    "    # 쿼리와 키 간의 내적을 계산합니다.\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1))  # (batch_size, num_queries, num_keys)\n",
    "    \n",
    "    # 어텐션 점수를 스케일링합니다.\n",
    "    d_k = key.size(-1)\n",
    "    attention_scores = attention_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # 제공된 경우 마스크를 적용합니다.\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # 2. 어텐션 가중치 계산\n",
    "    # 어텐션 점수에 소프트맥스 함수를 적용하여 어텐션 가중치를 얻습니다.\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, num_queries, num_keys)\n",
    "    \n",
    "    # 3. 출력 계산\n",
    "    # 값의 가중 평균을 계산하여 출력을 얻습니다.\n",
    "    output = torch.matmul(attention_weights, value)  # (batch_size, num_queries, d_v) , 행렬의 곱셈 = 행렬의 내적\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 예제 사용:\n",
    "batch_size = 32\n",
    "num_queries = 10\n",
    "num_keys = 100\n",
    "d_k = 64\n",
    "d_v = 32\n",
    "\n",
    "query = torch.randn(batch_size, num_queries, d_k)\n",
    "print(\"Shape of Q:\",query.shape)\n",
    "\n",
    "#키와 쿼리의 개수는 동일,벨류의 개수는 다름\n",
    "#Q,K,V 는 모두 같은 인풋(임베딩 됨)에서 나옴, but가중치 행렬이 적용되어 사이즈가 달라짐\n",
    "key = torch.randn(batch_size, num_keys, d_k)\n",
    "print(\"Shape of K:\",key.shape)\n",
    "value = torch.randn(batch_size, num_keys, d_v)\n",
    "print(\"Shape of V:\",value.shape)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "print(output.shape)  # 예상: torch.Size([32, 10, 64])\n",
    "print(attention_weights.size)  # 예상: torch.Size([32, 10, 100])\n",
    "print((attention_weights[0][:][:]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (src_embedding): Embedding(9998, 512)\n",
      "  (trg_embedding): Embedding(9998, 512)\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.5, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.5, inplace=False)\n",
      "          (dropout2): Dropout(p=0.5, inplace=False)\n",
      "          (dropout3): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=9998, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, nhead, num_layers, max_len, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # 소스 단어 임베딩\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        # 타겟 단어 임베딩\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
    "        # 위치 인코딩 생성\n",
    "        self.positional_encoding = self._generate_positional_encoding(d_model, max_len)\n",
    "        \n",
    "        # Transformer 모델 정의\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers, num_layers, dim_feedforward=d_model*4, dropout=dropout)\n",
    "        # 출력 레이어\n",
    "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
    "        \n",
    "    def _generate_positional_encoding(self, d_model, max_len):\n",
    "        # 위치 인코딩 행렬 생성\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # 소스 단어 임베딩과 위치 인코딩을 더함\n",
    "        src_emb = self.src_embedding(src) + self.positional_encoding[:src.size(0), :]\n",
    "        # 타겟 단어 임베딩과 위치 인코딩을 더함\n",
    "        trg_emb = self.trg_embedding(trg) + self.positional_encoding[:trg.size(0), :]\n",
    "        \n",
    "        # Transformer 모델의 forward 연산\n",
    "        output = self.transformer(src_emb, trg_emb)\n",
    "        # 출력 레이어를 통과시킴\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# 예제\n",
    "SRC_VOCAB_SIZE = 9998\n",
    "TRG_VOCAB_SIZE = 9998\n",
    "D_MODEL = 512\n",
    "NHEAD = 8\n",
    "NUM_LAYERS = 6\n",
    "MAX_LEN = 100\n",
    "\n",
    "# TransformerModel 인스턴스 생성\n",
    "model = TransformerModel(SRC_VOCAB_SIZE, TRG_VOCAB_SIZE, D_MODEL, NHEAD, NUM_LAYERS, MAX_LEN)\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
