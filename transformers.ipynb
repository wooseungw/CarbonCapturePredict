{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 트랜스포머 모델의 개괄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 트랜스포머란?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽이 디코더(Decoder) 레이어, 오른쪽이 인코더(Encoder) 레이어\n",
    "\n",
    "셀프 어텐션과 멀티 헤드 어텐션을 수행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 어텐션 메커니즘(Attention Mechanism)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 트랜스포머 구조를 이해하기 위해 **어텐션 메커니즘**을 이해할 필요가 있음\n",
    "\n",
    "CNN을 알기위해 Convolution을 알아야 하는것 처럼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. 기본 아이디어\n",
    "\n",
    "디코더에서 출력 예측하는 시점마다 인코더의 전체 입력을 다시 참고함\n",
    "\n",
    "여기서 전체 입력 중 예측하는 부분과 연관된 부분에 더 집중(Attenrion)해서 보기 때문에 \n",
    "\n",
    "    어텐션 메커니즘(Attention Mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 구성요소\n",
    "\n",
    "    아래의 설명은 Self Attention을 기준으로 함\n",
    "    쿼리(Query), 키(Key), 벨류(Value)\n",
    "\n",
    "위의 3가지 구성요소는 입력데이터($x$)에 대한 \"선형변환\" **(벡터와 행렬의 곱) $ = [vector][matrix]$** 으로 생성됨\n",
    "$$Q(Query) = xW_Q$$\n",
    "$$K(Key) = xW_K$$\n",
    "$$V(Value) = xW_V$$\n",
    "\n",
    "$W_Q,W_K,W_V$는 가중치 행렬이며 모델이 학습하며 업데이트하는 값\n",
    "\n",
    "$W_Q,W_K,W_V$는 임베딩 차원($d_{model}$)과 헤드의 개수($num\\;head$)에 기반해 생성\n",
    "\n",
    "임베딩 차원과 헤드가 뭔지는 트랜스포머 모델을 설명할 때\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 임베딩\n",
    "입력 데이터에 대한 고차원의 연속적인 특징 공간\n",
    "\"The cat sat on the mat\"를 입력 데이터라 할 때 $d_{model} = 512$ 라고 하자\n",
    "\n",
    "입력데이터에 대한 임베딩 결과는 $$x.shape = [6,512] = [입력데이터의\\;길이,d_{model}]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 결과의 크기 torch.Size([6, 512])\n",
      "임베딩 결과 tensor([[0.9851, 0.7044, 0.6417,  ..., 0.3498, 0.6813, 0.5172],\n",
      "        [0.7140, 0.8189, 0.6091,  ..., 0.1643, 0.1062, 0.4383],\n",
      "        [0.8386, 0.8516, 0.8611,  ..., 0.6039, 0.0629, 0.0511],\n",
      "        [0.0476, 0.0713, 0.8978,  ..., 0.0592, 0.2063, 0.8426],\n",
      "        [0.8651, 0.2321, 0.9658,  ..., 0.6976, 0.5677, 0.4458],\n",
      "        [0.4409, 0.8931, 0.8318,  ..., 0.0290, 0.9022, 0.8148]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = \"The cat sat on the mat\"\n",
    "d_model = 512\n",
    "\n",
    "def embedding(data,d_model):\n",
    "    #단어가 입력된 상황을 가정하고 토큰화 진행\n",
    "    tokens = data.split()\n",
    "    #토큰의 개수, d_model의 임베딩 생성됨\n",
    "    return torch.rand(len(tokens), d_model)\n",
    "\n",
    "embedding_matrix  = embedding(data,d_model)\n",
    "print(\"임베딩 결과의 크기\",embedding_matrix.shape)\n",
    "print(\"임베딩 결과\",embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 멀티-헤드\n",
    "\n",
    "트렌스포머 모델은 입력 데이터를 여러 헤드에서 동시에 처리함\n",
    "각 헤드마다 다른 관점에서 분석\n",
    "\n",
    "입력 데이터에 임베딩에는 사용되지 않음\n",
    "\n",
    "쿼리,키,벨류의 가중치 행렬 생성시 $shape = [d_{model},{d_{model} \\over num\\;head}]$으로 생성함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 쿼리(Query)\n",
    "\n",
    "현재 단어를 나타내는 벡터를 의미함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쿼리의 가중치 행렬\n",
    "\n",
    "$$W_Q = \\begin{bmatrix}\n",
    "q_{11} & \\cdots & q_{1(num\\;head)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_{(d_{model})1} & \\cdots & q_{(d_{model})(num\\;head)}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "쿼리 가중치 행렬의 크기\n",
    "$$W_Q.shape = [d_{model},{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "Q의 크기\n",
    "$$Q.shape = [입력데이터의\\;길이,{d_{model} \\over num\\;head}]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 예시\n",
    "\"The cat sat on the mat\"에서 \"cat\"이라는 단어를 분석한다고 가정할 때,$d_{model} = 512,num\\;head=8 $ 이라고 하자\n",
    "\n",
    " \"cat\"에 대응되는 **쿼리 벡터**를 찾고싶다.\n",
    "\n",
    "\n",
    "\n",
    "입력 데이터의 임베딩($x$)를 $x = \\begin{bmatrix}x_{1,1} & \\cdots & x_{1,512}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{6,1} & \\cdots & x_{6,512}\\end{bmatrix}$라 하자,\n",
    "\n",
    " 임의의 $W_Q = \\begin{bmatrix}\n",
    "q_{1,1} & \\cdots & q_{1,64}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_{512,1} & \\cdots & q_{512,64}\\\\\n",
    "\\end{bmatrix}$가 있을 때,\n",
    " \n",
    "  선형변환($=xW_Q$) 된 결과\n",
    "  \n",
    "$$Q =\n",
    "\\begin{bmatrix}x_{1,1} & \\cdots & x_{1,512}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{6,1} & \\cdots & x_{6,512}\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "q_{1,1} & \\cdots & q_{1,64}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "q_{512,1} & \\cdots & q_{512,64}\\\\\n",
    "\\end{bmatrix} \n",
    "=\\begin{bmatrix}\n",
    "(x_{1,1})(q_{1,1}) +\\cdots + (x_{1,512})(q_{512,1})& \\cdots & (x_{1,1})(q_{1,64}) +\\cdots +(x_{1,512}q_{512,64}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "(x_{6,1})(q_{1,1}) +\\cdots + (x_{1,512})(q_{512,1})& \\cdots & (x_{6,1})(q_{1,64}) +\\cdots + (x_{6,512})(q_{512,64})\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$Q.shape = [6,64] = [입력데이터길이,{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "$$\\begin{bmatrix} (x_{2,1})(q_{1,2}) +\\cdots + (x_{2,512})(q_{512,2}) & ,\\;\\cdots & ,(x_{2,1})(q_{1,64}) +\\cdots + (x_{2,512})(q_{512,64})\\end{bmatrix}\\;이\\;\"cat\"에\\;대한\\;쿼리 벡터\\; 이다.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 임베딩시 벡터의 크기 torch.Size([6, 512])\n",
      "쿼리 가중치 행렬의 크기 torch.Size([512, 64])\n",
      "\n",
      "cat에 대한 쿼리 벡터의 크기: torch.Size([64])\n",
      "cat에 대한 쿼리 벡터: tensor([126.7530, 126.7477, 125.9892, 125.6708, 121.0673, 118.2242, 120.0349,\n",
      "        126.1817, 121.5843, 127.4763, 125.1348, 117.7800, 125.4988, 127.0261,\n",
      "        122.9178, 123.5058, 122.0167, 126.1502, 124.4548, 121.0811, 129.9183,\n",
      "        124.8791, 125.2189, 125.8380, 128.4730, 127.3037, 124.0913, 125.2077,\n",
      "        117.9322, 130.1100, 126.5133, 119.5291, 123.4935, 133.0323, 115.3479,\n",
      "        129.5890, 126.6831, 119.4234, 123.9116, 126.4938, 126.4543, 128.1191,\n",
      "        129.0816, 117.0037, 119.4417, 122.7524, 128.6954, 120.4455, 117.2934,\n",
      "        122.0824, 129.8965, 126.2648, 124.1676, 127.9894, 125.1102, 120.4204,\n",
      "        120.0303, 119.3966, 124.3703, 120.8507, 120.1961, 120.5619, 124.1685,\n",
      "        126.2433])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 설정\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "# 랜덤함수로 임의의 임베딩 벡터 및 가중치 행렬 생성\n",
    "embedding_matrix = embedding(data,d_model)  # 6개 단어에 대한 임베딩\n",
    "W_q = torch.rand(d_model, head_dim)  # 쿼리 가중치 행렬\n",
    "\n",
    "print(\"문장 임베딩시 벡터의 크기\",embedding_matrix.shape)\n",
    "print(\"쿼리 가중치 행렬의 크기\",W_q.shape)\n",
    "# \"cat\"에 해당하는 임베딩 벡터 (예제에서는 두 번째 단어로 가정)\n",
    "cat_embedding = embedding_matrix[1, :]  # \"cat\"의 임베딩\n",
    "\n",
    "# \"cat\"에 대한 쿼리 벡터 계산\n",
    "cat_query = torch.matmul(cat_embedding, W_q)\n",
    "print()\n",
    "print(\"cat에 대한 쿼리 벡터의 크기:\", cat_query.shape)\n",
    "print(\"cat에 대한 쿼리 벡터:\", cat_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 키(Key) - 벨류(Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본개념: 키(Key) - 벨류(Value) 구조는 흔히 아래와 같은 딕셔너리 형에서 사용됨\n",
    "\n",
    "    키값으로 벨류를 가져올 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n"
     ]
    }
   ],
   "source": [
    "dict = {\"2017\" : \"Transformer\", \"2018\" : \"BERT\"}\n",
    "print(dict[\"2018\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "키(Key)가중치 행렬 \n",
    "\n",
    "$$W_K = \\begin{bmatrix}\n",
    "k_{11} & \\cdots & k_{1(num\\;head)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "k_{(d_{model})1} & \\cdots & k_{(d_{model})(num\\;head)}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "가중치 행렬의 크기\n",
    "$$W_K.shape = [d_{model},{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "K의 크기\n",
    "$$K.shape = [입력데이터의\\;길이,{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "벨류(Value)가중치 행렬 \n",
    "\n",
    "$$W_V = \\begin{bmatrix}\n",
    "v_{11} & \\cdots & v_{1(num\\;head)}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "v_{(d_{model})1} & \\cdots & v_{(d_{model})(num\\;head)}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "가중치 행렬의 크기\n",
    "$$W_V.shape = [d_{model},{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "V의 크기\n",
    "$$V.shape = [입력데이터의\\;길이,{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "#### 쿼리,키,벨류의 가중치 행렬은 모두 같은 크기를 가진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Attention\n",
    "### 2.5.1 Attention Score 계산\n",
    "어텐션 스코어는 쿼리와 키의 관계를 나타낸다.\n",
    "\n",
    "입력 시퀀스(=현재 쿼리)의 대한 전체요소들과의 가중치를 계산하는 것이다.\n",
    "\n",
    "이 계산 방법에 따라 **다양한 방법**으로 구현 될 수 있다. 그 중 내적(Dot-Product)을 이용한 계산을 알아보자면\n",
    "$$ Attention(Q,K,V) = softmax({QK^T \\over \\sqrt{d_k}}) $$\n",
    "로 정의되며 싱글 헤드 어텐션과 멀티 헤드 어텐션일때 계산이 달라진다.\n",
    "\n",
    " - **멀티-헤드 어텐션**\n",
    "$$ Attention(Q,K,V) = softmax({QK^T \\over \\sqrt{d_k}}) = softmax({QK^T \\over \\sqrt{{d_{model} \\over num\\;head}}})$$\n",
    "\n",
    " - 싱글-헤드셀프 어텐션(별로 중요하지 않아보인다. )\n",
    " $$ Attention(Q,K,V)= softmax({QK^T \\over \\sqrt{d_k}}) = softmax({QK^T \\over \\sqrt{{d_{model} \\over 1}}}) = softmax({QK^T \\over \\sqrt{d_{model}}}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 임베딩시 벡터의 크기 torch.Size([6, 512])\n",
      "쿼리 가중치 행렬의 크기 torch.Size([512, 64])\n",
      "키 가중치 행렬의 크기 torch.Size([512, 64])\n",
      "\n",
      "cat에 대한 어텐션 스코어 크기: torch.Size([6])\n",
      "cat에 대한 어텐션 스코어: tensor([0., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#어텐션 스코어 계산 예제\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "# 설정\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "# 랜덤함수로 임의의 임베딩 벡터 및 가중치 행렬 생성\n",
    "embedding_matrix = embedding(data,d_model)  # 6개 단어에 대한 임베딩\n",
    "W_q = torch.rand(d_model, head_dim)  # 쿼리 가중치 행렬\n",
    "\n",
    "print(\"문장 임베딩시 벡터의 크기\",embedding_matrix.shape)\n",
    "print(\"쿼리 가중치 행렬의 크기\",W_q.shape)\n",
    "# \"cat\"에 해당하는 임베딩 벡터 (예제에서는 두 번째 단어로 가정)\n",
    "cat_embedding = embedding_matrix[1, :]  # \"cat\"의 임베딩\n",
    "\n",
    "# \"cat\"에 대한 쿼리 벡터 계산\n",
    "cat_query = torch.matmul(cat_embedding, W_q)\n",
    "###################위와 동일#########################\n",
    "# 랜덤함수로 임의의 임베딩 벡터 및 가중치 행렬 생성\n",
    "W_k = torch.rand(d_model, head_dim)  # 키 가중치 행렬\n",
    "print(\"키 가중치 행렬의 크기\",W_q.shape)\n",
    "keys = torch.matmul(embedding_matrix, W_k)\n",
    "\n",
    "# 어텐션 스코어 계산\n",
    "attention_scores = torch.matmul(cat_query, keys.T) / torch.sqrt(torch.tensor(d_model // num_heads, dtype=torch.float32))\n",
    "attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "print()\n",
    "print(\"cat에 대한 어텐션 스코어 크기:\", attention_scores.shape)\n",
    "print(\"cat에 대한 어텐션 스코어:\", attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Attention Value 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어텐션 스코어를 바탕으로 각 벨류에서의 가중치를 부여해 쿼리벡터 하나에서 다른 요소에 대한 **의미**나 **특성** 그리고 **맥락**을 포함한다.\n",
    "\n",
    "어텐션 스코어에 가중치가 부여된 벨류를 합산한다.\n",
    "\n",
    "$$ Weighted\\;Value = Attention\\;Score\\times V  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예시\n",
    "앞서 구한 \"cat\"에 대한 어텐션 스코어를 바탕으로 \"cat\"에 대한 어텐션 벨류는? (d_model = 512, num head = 8)\n",
    "\n",
    "벨류벡터는 앞선 쿼리,키와 같이 데이터 임베딩 벡터와 벨류 가중치 행렬의 곱으로 나타난다 \n",
    "$$xW_V = \\begin{bmatrix}x_{1,1} & \\cdots & x_{1,512}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{6,1} & \\cdots & x_{6,512}\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{1,1} & \\cdots & v_{1,64}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "v_{512,1} & \\cdots & v_{512,64}\\\\\n",
    "\\end{bmatrix} \n",
    "=\\begin{bmatrix}\n",
    "(x_{1,1})(v_{1,1}) +\\cdots + (x_{1,512})(v_{512,1})& \\cdots & (x_{1,1})(v_{1,64}) +\\cdots +(x_{1,512}v_{512,64}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "(x_{6,1})(q_{1,1}) +\\cdots + (x_{1,512})(v_{512,1})& \\cdots & (x_{6,1})(v_{1,64}) +\\cdots + (x_{6,512})(v_{512,64})\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$V.shape = [6,64] = [입력데이터길이,{d_{model} \\over num\\;head}]$$\n",
    "\n",
    "우리가 원하는건 \"cat\"에 대한 어텐션 벨류 이므로\n",
    "\n",
    "$$(임의값 사용)\\begin{bmatrix} 0&,0&,0&,0&,1&,0\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "(x_{1,1})(v_{1,1}) +\\cdots + (x_{1,512})(v_{512,1})& \\cdots & (x_{1,1})(v_{1,64}) +\\cdots +(x_{1,512}v_{512,64}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "(x_{6,1})(q_{1,1}) +\\cdots + (x_{1,512})(v_{512,1})& \\cdots & (x_{6,1})(v_{1,64}) +\\cdots + (x_{6,512})(v_{512,64})\\\\\n",
    "\\end{bmatrix} \\;이\\;\"cat\"에\\;대한\\;어텐션\\; 벨류\\; 이다.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat에 대한 어텐션 벨류 크기: torch.Size([64])\n",
      "cat에 대한 어텐션 벨류: tensor([127.7655, 125.9784, 119.8620, 127.0230, 129.0132, 131.1996, 129.9930,\n",
      "        126.8404, 130.2188, 130.1385, 130.6226, 129.0536, 128.1087, 131.8503,\n",
      "        131.7267, 130.3544, 129.9814, 125.5734, 126.7550, 130.5771, 125.3661,\n",
      "        130.1339, 126.5389, 128.8256, 132.1857, 124.3674, 126.7825, 134.0955,\n",
      "        125.8793, 129.5358, 123.1817, 127.0538, 119.0791, 128.5627, 127.5258,\n",
      "        130.8436, 131.5439, 129.2166, 127.0407, 133.3909, 122.8754, 130.5598,\n",
      "        123.9540, 133.4200, 133.7713, 130.0197, 127.8870, 133.4268, 121.3879,\n",
      "        126.7154, 130.1044, 126.4273, 131.8867, 132.6349, 132.6805, 132.7206,\n",
      "        132.5424, 136.7787, 130.5218, 123.7178, 129.6644, 132.9768, 125.6332,\n",
      "        126.9372])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 설정\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "# 임의의 임베딩 및 가중치 행렬 생성\n",
    "embedding_matrix = torch.rand(6, d_model)  # 6개 단어에 대한 임베딩\n",
    "W_q = torch.rand(d_model, head_dim)  # 쿼리 가중치 행렬\n",
    "W_k = torch.rand(d_model, head_dim)  # 키 가중치 행렬\n",
    "W_v = torch.rand(d_model, head_dim)  # 밸류 가중치 행렬\n",
    "\n",
    "# \"cat\"에 해당하는 임베딩 벡터 (예제에서는 두 번째 단어로 가정)\n",
    "cat_embedding = embedding_matrix[1, :]  # \"cat\"의 임베딩\n",
    "\n",
    "# 쿼리, 키, 밸류 벡터 계산\n",
    "cat_query = torch.matmul(cat_embedding, W_q)\n",
    "keys = torch.matmul(embedding_matrix, W_k)\n",
    "values = torch.matmul(embedding_matrix, W_v)\n",
    "\n",
    "# \"cat\"에 대한 어텐션 스코어 계산\n",
    "attention_scores = torch.matmul(cat_query, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# \"cat\"에 대한 어텐션 벨류 계산\n",
    "cat_attention_values = torch.matmul(attention_scores, values)\n",
    "\n",
    "print(\"cat에 대한 어텐션 벨류 크기:\", cat_attention_values.shape)\n",
    "print(\"cat에 대한 어텐션 벨류:\", cat_attention_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어텐션 벨류 크기: torch.Size([6, 64])\n",
      "어텐션 벨류: tensor([[130.7679, 131.2600, 128.8020, 137.8293, 131.6392, 131.2069, 128.7494,\n",
      "         134.9128, 133.2825, 126.7834, 135.1894, 131.9203, 130.7347, 130.6917,\n",
      "         131.1009, 133.5622, 129.8923, 129.5577, 130.1403, 127.2794, 129.7968,\n",
      "         132.4955, 126.9975, 133.9855, 128.3091, 127.2353, 134.4391, 135.5217,\n",
      "         129.3004, 135.8638, 134.1209, 125.3821, 130.9818, 137.0243, 130.1015,\n",
      "         125.1560, 125.8326, 131.4495, 135.0916, 131.1440, 134.2398, 126.6428,\n",
      "         131.7151, 125.8072, 131.8732, 128.0460, 131.9175, 131.4975, 131.8590,\n",
      "         134.6504, 137.2288, 132.2314, 125.7504, 128.6073, 126.9401, 128.0023,\n",
      "         129.5443, 125.4113, 133.1927, 129.3299, 131.8839, 134.9148, 128.5634,\n",
      "         132.7564],\n",
      "        [130.7679, 131.2600, 128.8020, 137.8293, 131.6392, 131.2069, 128.7494,\n",
      "         134.9128, 133.2825, 126.7834, 135.1894, 131.9203, 130.7347, 130.6917,\n",
      "         131.1009, 133.5622, 129.8923, 129.5577, 130.1403, 127.2794, 129.7968,\n",
      "         132.4955, 126.9975, 133.9855, 128.3091, 127.2353, 134.4391, 135.5217,\n",
      "         129.3004, 135.8638, 134.1209, 125.3821, 130.9818, 137.0243, 130.1015,\n",
      "         125.1560, 125.8326, 131.4495, 135.0916, 131.1440, 134.2398, 126.6428,\n",
      "         131.7151, 125.8072, 131.8732, 128.0460, 131.9175, 131.4975, 131.8590,\n",
      "         134.6504, 137.2288, 132.2314, 125.7504, 128.6073, 126.9401, 128.0023,\n",
      "         129.5443, 125.4113, 133.1927, 129.3299, 131.8839, 134.9148, 128.5634,\n",
      "         132.7564],\n",
      "        [130.7679, 131.2600, 128.8020, 137.8293, 131.6392, 131.2069, 128.7494,\n",
      "         134.9128, 133.2825, 126.7834, 135.1894, 131.9203, 130.7347, 130.6917,\n",
      "         131.1009, 133.5622, 129.8923, 129.5577, 130.1403, 127.2794, 129.7968,\n",
      "         132.4955, 126.9975, 133.9855, 128.3091, 127.2353, 134.4391, 135.5217,\n",
      "         129.3004, 135.8638, 134.1209, 125.3821, 130.9818, 137.0243, 130.1015,\n",
      "         125.1560, 125.8326, 131.4495, 135.0916, 131.1440, 134.2398, 126.6428,\n",
      "         131.7151, 125.8072, 131.8732, 128.0460, 131.9175, 131.4975, 131.8590,\n",
      "         134.6504, 137.2288, 132.2314, 125.7504, 128.6073, 126.9401, 128.0023,\n",
      "         129.5443, 125.4113, 133.1927, 129.3299, 131.8839, 134.9148, 128.5634,\n",
      "         132.7564],\n",
      "        [130.7679, 131.2600, 128.8020, 137.8293, 131.6392, 131.2069, 128.7494,\n",
      "         134.9128, 133.2825, 126.7834, 135.1894, 131.9203, 130.7347, 130.6917,\n",
      "         131.1009, 133.5622, 129.8923, 129.5577, 130.1403, 127.2794, 129.7968,\n",
      "         132.4955, 126.9975, 133.9855, 128.3091, 127.2353, 134.4391, 135.5217,\n",
      "         129.3004, 135.8638, 134.1209, 125.3821, 130.9818, 137.0243, 130.1015,\n",
      "         125.1560, 125.8326, 131.4495, 135.0916, 131.1440, 134.2398, 126.6428,\n",
      "         131.7151, 125.8072, 131.8732, 128.0460, 131.9175, 131.4975, 131.8590,\n",
      "         134.6504, 137.2288, 132.2314, 125.7504, 128.6073, 126.9401, 128.0023,\n",
      "         129.5443, 125.4113, 133.1927, 129.3299, 131.8839, 134.9148, 128.5634,\n",
      "         132.7564],\n",
      "        [130.7679, 131.2600, 128.8020, 137.8293, 131.6392, 131.2069, 128.7494,\n",
      "         134.9128, 133.2825, 126.7834, 135.1894, 131.9203, 130.7347, 130.6917,\n",
      "         131.1009, 133.5622, 129.8923, 129.5577, 130.1403, 127.2794, 129.7968,\n",
      "         132.4955, 126.9975, 133.9855, 128.3091, 127.2353, 134.4391, 135.5217,\n",
      "         129.3004, 135.8638, 134.1209, 125.3821, 130.9818, 137.0243, 130.1015,\n",
      "         125.1560, 125.8326, 131.4495, 135.0916, 131.1440, 134.2398, 126.6428,\n",
      "         131.7151, 125.8072, 131.8732, 128.0460, 131.9175, 131.4975, 131.8590,\n",
      "         134.6504, 137.2288, 132.2314, 125.7504, 128.6073, 126.9401, 128.0023,\n",
      "         129.5443, 125.4113, 133.1927, 129.3299, 131.8839, 134.9148, 128.5634,\n",
      "         132.7564],\n",
      "        [130.7679, 131.2600, 128.8020, 137.8293, 131.6392, 131.2069, 128.7494,\n",
      "         134.9128, 133.2825, 126.7834, 135.1894, 131.9203, 130.7347, 130.6917,\n",
      "         131.1009, 133.5622, 129.8923, 129.5577, 130.1403, 127.2794, 129.7968,\n",
      "         132.4955, 126.9975, 133.9855, 128.3091, 127.2353, 134.4391, 135.5217,\n",
      "         129.3004, 135.8638, 134.1209, 125.3821, 130.9818, 137.0243, 130.1015,\n",
      "         125.1560, 125.8326, 131.4495, 135.0916, 131.1440, 134.2398, 126.6428,\n",
      "         131.7151, 125.8072, 131.8732, 128.0460, 131.9175, 131.4975, 131.8590,\n",
      "         134.6504, 137.2288, 132.2314, 125.7504, 128.6073, 126.9401, 128.0023,\n",
      "         129.5443, 125.4113, 133.1927, 129.3299, 131.8839, 134.9148, 128.5634,\n",
      "         132.7564]])\n"
     ]
    }
   ],
   "source": [
    "# 전체적인 과정 앞선 예제들은 cat 단어에 대한 예시였음\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 설정\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "\n",
    "# 임의의 임베딩 및 가중치 행렬 생성\n",
    "embedding_matrix =  embedding(data,d_model)  # 6개 단어에 대한 임베딩\n",
    "W_q = torch.rand(d_model, head_dim)  # 쿼리 가중치 행렬\n",
    "W_k = torch.rand(d_model, head_dim)  # 키 가중치 행렬\n",
    "W_v = torch.rand(d_model, head_dim)  # 밸류 가중치 행렬\n",
    "\n",
    "# 쿼리, 키, 밸류 벡터 계산\n",
    "queries = torch.matmul(embedding_matrix, W_q)\n",
    "keys = torch.matmul(embedding_matrix, W_k)\n",
    "values = torch.matmul(embedding_matrix, W_v)\n",
    "\n",
    "# 어텐션 스코어 계산\n",
    "attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n",
    "attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# 어텐션 벨류 계산\n",
    "attention_values = torch.matmul(attention_scores, values)\n",
    "\n",
    "print(\"어텐션 벨류 크기:\", attention_values.shape)\n",
    "print(\"어텐션 벨류:\", attention_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 트랜스포머 모델\n",
    "\n",
    "앞서 설명한 멀티 헤드 어텐션 개념으로 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 임베딩 차원과 멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 하는것은 입력데이터를 고정된 크기로 바꾸는 과정이다.\n",
    "위의 예시처럼 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[6, 512, 8, 64]' is invalid for input of size 3072",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m multi_head_attn \u001b[38;5;241m=\u001b[39m MultiHeadAttention(embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# 멀티-헤드 어텐션 수행\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_head_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\transformer\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 26\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, values, keys, query, mask)\u001b[0m\n\u001b[0;32m     23\u001b[0m value_len, key_len, query_len \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], keys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], query\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Split the embedding into self.heads different pieces\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m keys \u001b[38;5;241m=\u001b[39m keys\u001b[38;5;241m.\u001b[39mreshape(N, key_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m     28\u001b[0m queries \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mreshape(N, query_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[6, 512, 8, 64]' is invalid for input of size 3072"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Einsum does matrix multiplication for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just a way to do batch matrix multiplication\n",
    "        attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # attention: (N, heads, query_len, key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = F.softmax(attention / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "# 임의의 임베딩 및 마스크 생성\n",
    "embedding_matrix = torch.rand(6, 512)  # 6개 단어에 대한 임베딩\n",
    "mask = None  # 필요한 경우 마스크를 사용할 수 있습니다.\n",
    "\n",
    "# 멀티-헤드 어텐션 인스턴스 생성\n",
    "multi_head_attn = MultiHeadAttention(embed_size=512, heads=8)\n",
    "\n",
    "# 멀티-헤드 어텐션 수행\n",
    "out = multi_head_attn(embedding_matrix, embedding_matrix, embedding_matrix, mask)\n",
    "\n",
    "print(out)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
